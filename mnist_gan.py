# -*- coding: utf-8 -*-
"""MNIST_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JTP3dcu_sp3f5e01YpkiDhBz4DUOh9yi
"""

from keras.datasets import mnist
from keras.layers import *
import numpy as np
from keras.layers.advanced_activations import LeakyReLU
from keras.models import Sequential,Model
from keras.optimizers import Adam
import math
import matplotlib.pyplot as plt

(X_train,_),(_,_) =mnist.load_data()

print(X_train.shape)

plt.imshow(X_train[0],cmap='gray')
plt.show()

#Normalize the data

X_train= (X_train.astype('float32')-127.5)/127.5
print(np.min(X_train))
print(np.max(X_train))

print(X_train.shape)

Total_epochs = 50
Batch_size = 256
no_of_batches = int(X_train.shape[0]/Batch_size)
Half_batch=128

#Basically in each epoch 128 fake and 128 real images will be given to the discriminator to discriminate.
Noise_dim=100 # we will upsample it to 784
adam= Adam(lr=2e-4,beta_1=0.5)

#Generator 
#Input noise 100 and upsample it to 784

generator = Sequential()
generator.add(Dense(256,input_shape=(Noise_dim,)))
generator.add(LeakyReLU(0.2))
generator.add(Dense(512))
generator.add(LeakyReLU(0.2))
generator.add(Dense(1024))
generator.add(LeakyReLU(0.2))
generator.add(Dense(784,activation='tanh'))

generator.compile(loss='binary_crossentropy' , optimizer=adam)
generator.summary()

#Now we do downsampling
#Discriminator

discriminator=Sequential()
discriminator.add(Dense(512,input_shape=(784,)))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dense(256))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dense(1,activation='sigmoid'))
discriminator.compile(loss='binary_crossentropy',optimizer=adam)
                  
discriminator.summary()

#Gan
discriminator.trainable = False
gan_input = Input(shape=(Noise_dim,))
generated_image=generator(gan_input)
gan_output=discriminator(generated_image)

#Functional API
model = Model(gan_input,gan_output)
model.compile(loss='binary_crossentropy',optimizer=adam)

X_train=X_train.reshape((-1,784))
print(X_train.shape)

!mkdir model

def save_images(epoch,samples=100):
  noise = np.random.normal(0,1,size=(samples,Noise_dim))
  generated_imgs=generator.predict(noise)
  generated_imgs=generated_imgs.reshape(samples,28,28)
  
  plt.figure(figsize=(10,10))
  for i in range(samples):
    plt.subplot(10,10,i+1)
    plt.imshow(generated_imgs[i],interpolation='nearest',cmap='gray')
    plt.axis('off')
    
  plt.tight_layout()
  plt.savefig('images/gan_output_epoch_{0}.png'.format(epoch+1))
  plt.show()

#Training loop
d_losses=[]
g_losses=[]

for epoch in range(Total_epochs):
  epoch_d_loss=0
  epoch_g_loss=0
  
  #Mini batch SGD
  for step in range(no_of_batches):
    #Step 1 is to train the discriminator
    #It has 50% real and 50% fake data
    idx = np.random.randint(0,X_train.shape[0],Half_batch)
    #This is random indices from the training data. 0 to 60000 with each idx of 128.128 is the number of images.
    #idx will be a list
    real_images=X_train[idx]
    
    #now fake data
    noise = np.random.normal(0,1,size=(Half_batch,Noise_dim))
    fake_images=generator.predict(noise) #Forward prop. This passes the noise vector through the generator and it generates fake images.
    
    #Now we should also assign labels to the data for discriminator to discriminate.
    real_y = np.ones((Half_batch,1))*0.9
    # this is one sided smoothing. multiplying each 1 by 0.9
    fake_y = np.zeros((Half_batch,1))
    
    #Train the discriminator
    d_loss_real  = discriminator.train_on_batch(real_images,real_y)
    d_loss_fake = discriminator.train_on_batch(fake_images,fake_y)
    d_loss = 0.5*d_loss_real + 0.5*d_loss_fake
    
    epoch_d_loss += d_loss
    
    #Train the generator considering the discriminator is frozen.
    noise = np.random.normal(0,1,size=(Batch_size,Noise_dim))
    # we want the ground truth to be 1 as the generator wants to generate images that the discriminator thinks is real.
    ground_truth_y = np.ones((Batch_size,1))
    g_loss = model.train_on_batch(noise,ground_truth_y)
    epoch_g_loss += g_loss
    
    #This completes the training
    
  print("Epoch %d Disc loss %.4f Generator loss %.4f"%((epoch+1),epoch_d_loss/no_of_batches,epoch_g_loss/no_of_batches))
  d_losses.append(epoch_d_loss/no_of_batches)
  g_losses.append(epoch_g_loss/no_of_batches)
  if (epoch%5==0):
    generator.save('model/gan_generator_{0}.h5'.format(epoch+1))
    save_images(epoch)

!mkdir images

!ls

plt.plot(d_losses,label='Disc')
plt.plot(g_losses,label='Gen')
plt.legend()
plt.show()

!zip -r /content/images.zip /content/images

from google.colab import files
files.download('images.zip')

"""# New Section"""